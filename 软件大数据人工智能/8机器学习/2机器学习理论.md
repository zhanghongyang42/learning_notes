吴恩达2022机器学习：https://www.bilibili.com/video/BV1Bq421A74G













大模型参考;https://space.bilibili.com/1232187625/upload/video

https://space.bilibili.com/1369507485/lists?sid=788485&spm_id_from=333.788.0.0

















# softmax

softmax回归是一种多分类算法



### 模型表示

y = softmax(wx + b)

其中，有n个分类，就有n组参数预测每个分类。n组参数放入 softmax，就可以把n个概率统一，使其相加为1

![image-20230323162346585](picture/image-20230323162346585.png)



### 代价函数

![image-20230323171334033](picture/image-20230323171334033.png)

softmax 代价函数是对每组参数分别计算代价的和
$$
J(W, b) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{n} [y_i^j \log(p_i^j)]
$$

- m：样本数量；
- n：类别数量；
- y_i^j：第i个样本的真实标签向量的第j个元素，取值为0或1；
- p_i^j：第i个样本的预测概率向量的第j个元素，由Softmax函数计算得出；



# ----  ----



# 神经网络

起源：模仿大脑的算法。

发展：1950开始有，1980末90初被使用，1990末不再使用，2005再次发展。

领域：语音-->图像-->文本（NLP）--> ......



### 模型定义

神经元（unit）：可以理解为一个简单的模型，可以有一个或几个输入，得到一个输出。

​								具体的，神经元一般由 线性计算+激活函数 组成。

参数（weight）：神经元中可以变化，需要训练的参数。



模型的各个层都是一组神经元组成的。神经网络的层数时不包括输入层。

输入层（Input Layer）：输入层只是一组输入数字，直接传给下一层。可以理解为特征。

隐藏层（Hidden Layers）：隐藏层是一组神经元。每个神经元的输出 可以理解为 特征中隐藏的信息 或者 特征中间预测结果。

输出层（Output Layer）：最后一组作为输出的神经元。



神经网络架构：多少个隐藏层和输出层，每层多少个神经元（unit）。

很多情况下，随着层数的增加，神经元减少的结构比较有效。

![image-20220818113346172](picture/image-20220818113346172.png)



forward propagation 正向传播：神经网络每一层从前往后计算。





https://www.bilibili.com/video/BV1Bq421A74G?vd_source=95e4d2371451c9804d5d5d30293ea8c6&spm_id_from=333.788.player.switch&p=46





### 直观理解

从单层神经元实现的功能来直观的理解神经网络。

使用单层神经元实现 逻辑与(AND)运算。

![image-20221230164456310](picture/image-20221230164456310.png)

也可以实现其他逻辑运算。

![image-20221230164753503](picture/image-20221230164753503.png)

实现异或功能

![image-20221230164909489](picture/image-20221230164909489.png)



### tensorflow

基本代码

tensor（张量）是tensorflow的输入数据的类型，类似于矩阵数组。

![image-20220818201333525](picture/image-20220818201333525.png)



使用sigmoid进行二分类代码，画线部分是对直接使用sigmoid激活函数的优化。

![image-20220819170645073](picture/image-20220819170645073.png)

Dense Layer：该层神经网络中的每个神经元都会接受所有的输入。



### 激活函数

ReLU：g(z) = max(0,z)

线性激活函数 ：g(z) = z

sigmoid：1 / (1 + e^(-x))

softmax：e^(x_i) / Σ(e^(x))



输出层激活函数的选择：

二分类 问题，输出层 激活函数通常使用 sigmoid。输出两个概率。

多分类 问题，输出层 激活函数通常使用 softmax。输出多个概率。

回归 问题，输出层 激活函数通常使用 线性激活函数。因为输出有正有负

回归问题，y只有正值，输出层 激活函数通常使用ReLU。输出永远非负



隐藏层 激活函数 通常使用 ReLU，一般绝对不会使用线性激活函数（会导致神经网络及其简单至失效）。



ReLU 比 sigmoid 更经常使用的原因：

1. ReLU计算复杂性低，速度更快。
2. 在使用梯度下降求损失函数最小时，sigmoid因为两侧都比较平滑，造成梯度下降速度缓慢。



为什么要使用激活函数：如果不使用激活函数，神经网络将变成线性回归函数，不再发挥作用。



# 神经网络的多分类

### 多分类

神经网络的多分类（multi-label classfication）。使用softmax激活函数实现。

softmax激活函数是特殊的，用到这一层所有神经元的线性计算结果。

![image-20230323185523772](picture/image-20230323185523772.png)



### 多标签分类

![image-20230323185523772](picture/image-20220819182200545.png)



# 神经网络求解（？）

训练神经网络：

1. 参数的随机初始化
2. 利用正向传播方法计算所有的
3. 编写计算代价函数  的代码
4. 利用反向传播方法计算所有偏导数
5. 利用数值检验方法检验这些偏导数
6. 使用优化算法来最小化代价函数



### 代价函数

逻辑回归的代价函数是二分类的交叉熵损失函数，此处神经网络的代价函数是多分类的交叉熵损失函数。

![image-20230103145638282](picture/image-20230103145638282.png)



### 反向传播

如不理解，可以继续看视频。

反向传播算法用来计算代价函数的偏导数。



前向传播算法

![image-20230103150517721](picture/image-20230103150517721.png)

反向传播算法

![image-20230103150801595](picture/image-20230103150801595.png)



### 梯度检验

略



### 随机初始化

神经网络的参数初始化为随机值。

https://zhuanlan.zhihu.com/p/41465442



# 神经网络优化

使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小

使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。

通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。



对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，

可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层 层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。





















# ---- ----



# 推荐系统

http://www.ai-start.com/ml2014/html/week9.html



# ---- ----



# 大规模机器学习

### 随机梯度下降法

http://www.ai-start.com/ml2014/html/week10.html 17.2 17.3 17.4





# 其他模型

## 关键词提取

关键词提取技术，包括有监督（部分打标签，训练，人工把预测正确关键词的数据再加入训练集，继续训练），无监督（TF-IDF、TextRank、基于主题）



### bag of words

词袋模型：把每篇文档出现的单词按照顺序排列，统计出每个token（一般就是一个单词）的出现次数，组成向量。

词袋模型忽略了词序语法近义等，每个词相互独立存在。

```python
corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names())
print(X.toarray())

#ngram_range默认1个词为1个token，可以设置1，2，3个词都是1个token，这样可以把词的顺序信息训练进来
bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))
```



### TF-IDF

关键词提取技术的一种，通过计算每个词在文章中出现的频率*该词的重要性来确定关键词。



TF-IDF = TF*IDF

![20180806135836378](https://raw.githubusercontent.com/zhanghongyang42/images/main/20180806135836378.png)

![20180806140023860](https://raw.githubusercontent.com/zhanghongyang42/images/main/20180806140023860.png)



明显看出，词袋模型的结果可以直接用来计算TF-IDF，所以实现为CountVectorizer+TfidfTransformer   或者  TfidfVectorizer。



##### TfidfTransformer

```python
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer(smooth_idf=False)

#词袋模型结果，一个列表代表一篇文章，共有6篇文章，每一列代表一个词，3代表第一个词在第一篇文章里出现了3次。
counts = [[3, 0, 1],
          [2, 0, 0],
          [3, 0, 0],
          [4, 0, 0],
          [3, 2, 0],
          [3, 0, 2]]

#每一篇文章的每一个词都可以计算出tf-idf，最后把结果归一化
tfidf = transformer.fit_transform(counts)
print(tfidf.toarray())
```



##### TfidfVectorizer

CountVectorizer  +  TfidfTransformer

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(corpus)
```



### TextRank

##### PageRank

TextRank 的思想来源于 PageRank。

PageRank：https://www.cnblogs.com/jpcflyer/p/11180263.html

```python
#PageRank 过程
给每个点（网页）初始权重。
写出转移矩阵（转移矩阵的理论基础就是那个pr计算公式，按改良公式进行转移得保证总量不变）。
根据转移矩阵更新初始权重。直至权重不再变化。
不再变化的权重就是那个网页的pr值（pr值不再变化的时候就完全反应了累积过后的【转移关系(用户意愿)】）。
```



##### TextRank

https://www.cnblogs.com/motohq/p/11887420.html

TextRank 与 PageRank相比，不同是每个词语之间都是双向边，且边有权重（双向边代表在同一句子中出现，边的权重代表出现次数或者相似度）。表现为 转移矩阵或者说理论公式不同。

 

本质是用词之间的关系找出关键词。所以 这种计算方法一定要过滤掉停用词。

```python
#TextRank 过程
1.把文本 按 句子进行分割
2.对每个句子进行 分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词。
3.按PageRank 过程 构建图，计算出每个词的权重。
4.选topN 作为 候选关键词。
5.看候选关键词是否在文本中可以组成关键词组，把关键词组也作为关键词。
```

```python
import pandas as pd
import numpy as np
from textrank4zh import TextRank4Keyword, TextRank4Sentence

# 关键词抽取
def keywords_extraction(text):
    tr4w = TextRank4Keyword(allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])
    # allow_speech_tags   --词性列表，用于过滤某些词性的词
    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',
                 pagerank_config={'alpha': 0.85, })
    # text    --  文本内容，字符串
    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2
    # lower   --  是否将英文文本转换为小写，默认值为False
    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点
    #                -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'
    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边
    #              -- 默认值为`'no_stop_words'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。边的构造要结合`window`参数

    # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85
    keywords = tr4w.get_keywords(num=6, word_min_len=2)
    # num           --  返回关键词数量
    # word_min_len  --  词的最小长度，默认值为1
    return keywords

# 关键短语抽取
def keyphrases_extraction(text):
    tr4w = TextRank4Keyword()
    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',
                 pagerank_config={'alpha': 0.85, })
    keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1)
    # keywords_num    --  抽取的关键词数量
    # min_occur_num   --  关键短语在文中的最少出现次数
    return keyphrases

# 关键句抽取
def keysentences_extraction(text):
    tr4s = TextRank4Sentence()
    tr4s.analyze(text, lower=True, source='all_filters')
    # text    -- 文本内容，字符串
    # lower   -- 是否将英文文本转换为小写，默认值为False
    # source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。
    # 		  -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'
    # sim_func -- 指定计算句子相似度的函数

    # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要
    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)
    return keysentences

if __name__ == "__main__":
    text = "来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，" \
           "我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、" \
           "副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”" \
           "据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，" \
           "获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，" \
           "国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，" \
           "正式将该小行星命名为“周又元星”。"
   
	# 关键词抽取
    keywords =keywords_extraction(text)
    print(keywords)

    # 关键短语抽取
    keyphrases =keyphrases_extraction(text)
    print(keyphrases)

    # 关键句抽取
    keysentences =keysentences_extraction(text)
    print(keysentences)
```



### 主题模型

主题模型是生成模型。生成模型与判别模型的区别在于：判别模型直接计算出概率，生成模型比较各个可能的概率，选最大的。

主题模型包括 LSA、pLSA、LDA、HDP、LDA2Vec。



##### LSA

https://zhuanlan.zhihu.com/p/46376672

```python
#LSA过程
1.首先计算出 TF-IDF
2.进行svd矩阵分解 特征向量
3.利用余弦相似性找出相似文本
```

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#读数据
from sklearn.datasets import fetch_20newsgroups
dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data
dataset.target_names

#删除符号与短词
news_df = pd.DataFrame({'document':documents})
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z#]", " ")
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())

#删除停用词
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])
detokenized_doc = []
for i in range(len(news_df)):
    t = ' '.join(tokenized_doc[i])
    detokenized_doc.append(t)
news_df['clean_doc'] = detokenized_doc

#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english',max_df = 0.5, max_features= 1000,smooth_idf=True)
X = vectorizer.fit_transform(news_df['clean_doc'])

# SVD
from sklearn.decomposition import TruncatedSVD
svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)

#主题排序
terms = vectorizer.get_feature_names()
for i, comp in enumerate(svd_model.components_):
    terms_comp = zip(terms, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]
    print("Topic "+str(i)+": ",end='')
    for t in sorted_terms:
        print(t[0],end=' ')
    print(" ")
```



## 赋权法

赋权法用于需要 确定权重系数的情况。分为主观赋权法和客观赋权法。

熵权法 是客观赋权法的一种，层次分析法是主观赋权法的一种。



### 熵权法

熵权法 计算  **信息熵**  来确定各个变量  **权重**  ，之后通过**变量和权重**可以计算出该条样本的  **评分**，用于挑选样本。

推导过程：https://www.zhihu.com/question/357680646/answer/943628631

```python
import pandas as pd
import numpy as np
import math
from numpy import array

df = pd.read_csv('aaa.csv')

#该方法要求x全为数值型
def cal_weight(x):
    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))
    # 求k
    rows = x.index.size  # 行
    cols = x.columns.size  # 列

    k = 1.0 / math.log(rows)

    lnf = [[None] * cols for i in range(rows)]
    # 矩阵计算--
    # 信息熵
    # p=array(p)
    x = array(x)
    lnf = [[None] * cols for i in range(rows)]
    lnf = array(lnf)

    for i in range(0, rows):
        for j in range(0, cols):
            if x[i][j] == 0:
                lnfij = 0.0
            else:
                p = x[i][j] / x.sum(axis=0)[j]
                lnfij = math.log(p) * p * (-k)
            lnf[i][j] = lnfij
    lnf = pd.DataFrame(lnf)
    E = lnf

    # 计算冗余度
    d = 1 - E.sum(axis=0)
    # 计算各指标的权重
    w = [[None] * 1 for i in range(cols)]
    for j in range(0, cols):
        wj = d[j] / sum(d)
        w[j] = wj
        # 计算各样本的综合得分,用最原始的数据

    w = pd.DataFrame(w)
    return w

w = cal_weight(df)
w.index = df.columns
w.columns = ['weight']

df['score'] = 0
for i in df.columns:
    if i != 'score':
        df['score'] = df['score'] + df[i]*w.loc[i,'weight']

# w 是每个特征的权重，df['score'] 是每条数据的得分
```



### 层次分析法

层次分析法本质上通过人的主观判断给变量不同的权重，然后再通过验证，最终确定权重。确定权重之后即可给每条数据打分。

使用流程：https://blog.csdn.net/lengxiao1993/article/details/19575261

```python
import numpy as np
import pandas as pd

# # 需要人手动给出比较矩阵
# compare_matrix = np.array([[1, 0.2, 0.33, 1],
#                           [5, 1, 1.66, 5],
#                           [3, 0.6, 1, 3],
#                           [1, 0.2, 0.33, 1]])

# # 一致性检验
# def isConsist(F):
#     n = np.shape(F)[0]
#     a, b = np.linalg.eig(F)
#     maxlam = a[0].real
#     CI = (maxlam - n) / (n - 1)
#     if CI < 0.1:
#         return bool(1)
#     else:
#         return bool(0)
# # 一致性检验异常
# class isConsistError(Exception):
#     def __init__(self, value):
#         self.value = value

#     def __str__(self):
#         return repr(self.value)

# if isConsist(compare_matrix) == False:
#     raise isConsistError('一致性检验不通过')

# # 根据相对矩阵，算出每一个影响因素的重要程度
# def ReImpo(F):
#     n = np.shape(F)[0]
#     W = np.zeros([1, n])
#     for i in range(n):
#         t = 1
#         for j in range(n):
#             t = F[i, j] * t
#         W[0, i] = t ** (1 / n)
#     W = W / sum(W[0, :])  # 归一化 W=[0.874,2.467,0.464]
#     return W.T
# W = ReImpo(compare_matrix)


# #此矩阵的每一行代表一个方案，每一列代表一个影响因素/此矩阵为方案层权重矩阵，大部分场景可以自动生成
# df = pd.read_csv('entropy_weight.csv')

# #归一化方案层权重矩阵
# for i in df:
#     df[i] = df[i]/sum(df[i])
    
    
# score = np.dot(df,W)
# score = pd.DataFrame(score)
# print(score)
```





















