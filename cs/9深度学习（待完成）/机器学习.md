https://www.youtube.com/@HungyiLeeNTU 2021机器学习



# 模型思路

![image-20250411173204836](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250411173204836.png)

调整思路：

```
训练集上损失大：

	1、模型复杂度不够

	2、优化方法有问题
	
	怎么分辨：使用简单的模型，简单模型一般不会存在优化不到位的问题，如果简单模型的损失更小，说明现在模型的优化方法有问题

	解决方法：

		1、使用更复杂的模型
		
		2、略

训练集损失小，测试集损失大：
		
	1、过拟合
		
	2、测试集出现了训练集没有的数据，分布不同
	
	怎么分辨：根据业务理解，是不是分布不同。
	
	解决方法：
		
		1、更多的训练数据、简化模型等。
```

建模思路：

```
矛盾问题：模型简单了会出现 model bias ，复杂了又会overfitting。怎么建模避免这个矛盾

建模过程：使用多个模型，每个模型都进行多则交叉验证（将训练集分为训练集和验证集），选择评价效果最好的那个模型。
		注意，不从模型预测得到的测试集结果调整模型。
```



# optimization

求目标函数最小值的优化方法，如梯度下降



train loss 不再下降原因：gradient 在 0 处收敛，即达到 critical point。

​	一、 局部最小值 (local minima)，无路可走。

​	二、鞍点 (saddle point)，仍有路可走，使loss降低。



区分 local minima 和 saddle point，了解即可：

![image-20250825192933065](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250825192933065.png)



实际上，维度越高，local minima 会变成 saddle point ，local minima 越少。所以卡在 saddle point 的情况是大部分。



# batch

1 **epoch** = see all the batches once，然后 Shuffle after each epoch



batch size 需要选取合适的数值：

​	一、batch size 越大，训练速度一般越快

​		*因为有GPU的并行运算，batch size 在一定大小范围内的训练时间是相同的，batch size 越大，update 次数越少，总时间越短。*

​	二、batch size 越小，train 的效果越好

​		*batch size 大，可能在某一次 update 就达到了 local minima。batch size 小，update 次数多，更不容易陷入到 local minima。*

​	三、batch size 越小，test 效果越好

​		*batch size 大，local minima 更容易狭窄，反之，batch size 小，会更容易达到比较宽阔的local minima。*

​		*因为 train 和 test 的参数本身就有差距，所以当 local minima 比较宽阔的时候，更不容易过拟合*



# momentum



# Learning Rate

不同的参数，不同的步数，需要不同的学习率：

一、使用学习率除以 σ 作为新的学习率，σ在 error surface 平缓的地方小，在陡峭的地方大。

![image-20250902182107864](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902182107864.png)

二、加入一个 α 项，可以调整学习率变化倾向于 最近或者之前 的梯度。

![image-20250902183946287](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902183946287.png)

三、Adam

![image-20250902184454227](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902184454227.png)

四、RAdam，根据不同的步数，调整学习率的两种方法。

![image-20250902185538235](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902185538235.png)



### 梯度下降的提升方式

![image-20250902190258779](https://raw.githubusercontent.com/zhanghongyang42/images/main/image-20250902190258779.png)



# Batch Normalization



**为什么要批次标准化**

当 x1，x2 范围差距过大，为了得到最佳的loss，w1，w2的差距也会很大。会得到一个很难达到 local minima 的 error surface。

所以，为了得到一个比较好的 error surface，需要对 x1，x2 进行 Normalization。



1























































